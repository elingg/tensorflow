{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "GO = chr(first_letter + 26)\n",
    "EOS = chr(first_letter + 27)\n",
    "\n",
    "def char2vector(chr):\n",
    "    char_id = char2id(chr)\n",
    "    return [1.0 * (i - char_id) for i in range(vocabulary_size)]\n",
    "\n",
    "def vector2id(vector):\n",
    "    return np.argmax(vector, 0)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "train_reversed_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_reversed_batches = BatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, 4* num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    gate = tf.matmul(i+i+i+i, ix) + tf.matmul(o+o+o+o, im) + ib\n",
    "    input_gate = tf.sigmoid(gate[:,0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(gate[:,num_nodes:num_nodes*2])\n",
    "    update = tf.sigmoid(gate[:,num_nodes*2:num_nodes*3])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gate[:,num_nodes*3:num_nodes*4])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.303383 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.20\n",
      "================================================================================\n",
      "ridqaekfiacu tzesve aetzsyr dwhbkedxsxiebayil widiijtvangjeypypeefl o njsys f  e\n",
      "mty tinexefiyaziagodni t yvxbeo ohgmqgkqt talpdmtsos dstatyzulexi dxry wqoossruc\n",
      "nbivlmnsuhxgomeiunwxotq loieu dnmsb rrtagpzdtwhuve mtjlogr peiain afywtuamwbboke\n",
      "aibhkuyjbi  faie hoof cega cdg lgd v bred odavtnjngmi cjrerycjilbimya ml ej  obp\n",
      "pcdjvmcgbeusbxnwy a qaii gaiphhhvr isd  msfoa rwe xcl  oefmrvixkkozvvsrtanjbwson\n",
      "================================================================================\n",
      "Validation set perplexity: 20.54\n",
      "Average loss at step 100: 2.633585 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 10.35\n",
      "Average loss at step 200: 2.262016 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.90\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 300: 2.140586 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 400: 2.058532 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 500: 2.000526 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 600: 1.975360 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 700: 1.921383 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 800: 1.886585 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 900: 1.888041 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 1000: 1.886149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "v kto denily matis filanfecting ferper of strust utional aved wils comallicalned\n",
      "vellf arreceming polted bullition in farel prost ased furty presuy indicial fro \n",
      "livial wan or fame reber noure ame and wibl fevest chyda clicte comofeth all kfe\n",
      "nitons to for binarty on onal nine ofteigne pre will fomexkensorked is clvicusem\n",
      "unisture offerees a repively earide goyver in the tie dua they sevell to othe hu\n",
      "================================================================================\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 1100: 1.835867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 1200: 1.812757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1300: 1.792327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1400: 1.803307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.800644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1600: 1.805917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1700: 1.772589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1800: 1.733967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1900: 1.711708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 2000: 1.756175 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "================================================================================\n",
      "am antinos one ninatur alman imeen of that mainst is inated c preas voning cfun \n",
      "e not ase to a in chaptaldd unueaciss macting his lineerabl i gilm to the did tw\n",
      "vigan vierattersess giestended paporscossimally worming was pittriving  shoorgan\n",
      " or but from from f dustron ulansalavald at that a zero zero zero seacions wopd \n",
      "fing trentanion on evol scumpandin msdician nols wars agi that then redon the le\n",
      "================================================================================\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 2100: 1.742830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2200: 1.735012 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2300: 1.692110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.712327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2500: 1.734701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2600: 1.711038 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2700: 1.705115 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2800: 1.706768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2900: 1.699993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3000: 1.696385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "udep and governilan an and the himmader to the unper sirs s and projecatrive two\n",
      "f to and zero satory revilales s are two six one one eight deese and he perached\n",
      "e commonative impol and creativels sancauser and vichers the nation s abshite an\n",
      "cley heple ameripangs fadla situalle two smints spricanty va buz lasbol stagm ie\n",
      "dacheus and five the nigutiogcarler and the that it less ims first follow two ze\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3100: 1.675189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3200: 1.694121 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3300: 1.683549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3400: 1.713689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3500: 1.700364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3600: 1.710916 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.691243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3800: 1.688178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3900: 1.674100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4000: 1.692157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "quum tech calur addus hensores mava rec preticide veristy the solugh wright from\n",
      "phibl remailleatal golangakizatich to zero two former by in the countable scanda\n",
      "e the discfeetions are sons addismwologon main the ent chigal deg general propos\n",
      "formartions broth leastilled yealth seven eight of the compeots ang bounish occe\n",
      "h fangdo hissossuen epapen citaliulation contribe permost the house one three ve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4100: 1.674010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4200: 1.680023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4300: 1.660686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4400: 1.652375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4500: 1.659839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4600: 1.657344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4700: 1.670347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4800: 1.675530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4900: 1.670365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.643908 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "quait or sallef of culs the was advically hather nople the two six nine six zero\n",
      "f a which yacus from the s had after lifger hulfh four nouth s book game all and\n",
      "y cornine a widies iv livic mase with yres becan versh is alfortary his of the w\n",
      "wited by marksaries more humonf who regults inexmendhs anhonos finy as be design\n",
      "ficime of dihothliving that milisher of the q first and ateerencifagy of a saign\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.643241 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.624804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.604973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400: 1.609212 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5500: 1.589817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5600: 1.609349 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5700: 1.594409 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5800: 1.606306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5900: 1.598811 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6000: 1.571694 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ing which organt groson to ship in lake eight perzendenam wast to cat of kledist\n",
      "hing trier would was a spone gremalies based by sa creading phinsion hadvamber r\n",
      "vident of the bosh sanpares and demazan renumune with port o duactiu expanimatio\n",
      "rivitualle jako dame a world fadity by these votive of jody for git lithoning we\n",
      "folbemination the pall the shoversions relative has public tem bor lett after ha\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.590505 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6200: 1.561613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6300: 1.570861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6400: 1.565079 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6500: 1.581764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.619148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6700: 1.600198 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6800: 1.623496 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6900: 1.603857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.599413 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "x genesoutly effece expanted synution the lines koutsauties candend and suter ka\n",
      "arcting preventor that but maw of a that about the will early afrownided game ca\n",
      " crenon sides of exeger c thin attentiogy abieriam in the one illeedant o qionat\n",
      "zer in the puphound of the belgepsca and astisenging became sirfething amproyed \n",
      "x or harriares and the temple mought stated seven fic their brainst ahewaylism i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embeddings = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  embeddings_vocab = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embeddings], -1.0, 1.0))\n",
    "  ix = tf.Variable(tf.truncated_normal([embeddings, 4 * num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, 4* num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    gate = tf.matmul(i+i+i+i, ix) + tf.matmul(o+o+o+o, im) + ib\n",
    "    input_gate = tf.sigmoid(gate[:,0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(gate[:,num_nodes:num_nodes*2])\n",
    "    update = tf.sigmoid(gate[:,num_nodes*2:num_nodes*3])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gate[:,num_nodes*3:num_nodes*4])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by two time steps.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(embeddings_vocab, index)\n",
    "    drop_i = tf.nn.dropout(i_embed, 0.7)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    drop_o = tf.nn.dropout(output, 0.7)\n",
    "    outputs.append(drop_o)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2): \n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  sample_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings_vocab, sample_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307633 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "================================================================================\n",
      "ty   he tghap q n   lkyc uo   ce txq belr w cezny   fpy tihi     cn   gn r s   yt\n",
      "tcs   fqgajtv e   ijb trbyioze y n   tnhnsg ve tcpog f uau   vwcndqe tiiwzpvorm e\n",
      "aoq zihityw dr legwbf uxloa ttei   inire lia   xps ixwutctu     eh lo  srjkfohv q\n",
      "ya    obat i hii p kd       peembd vpii bwvn r kjpie ei ep oe ee   eg pg klvgq ah\n",
      "lxltdqd m y xs euzcrn hbs bm tp cps j w lg k pnho tn elr y jb tb   dl bvrpnptzesa\n",
      "================================================================================\n",
      "Validation set perplexity: 23.51\n",
      "Average loss at step 100: 2.583876 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 10.19\n",
      "Average loss at step 200: 2.291254 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.77\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 300: 2.231961 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.15\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 400: 2.210199 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.48\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 500: 2.188007 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 600: 2.142133 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 700: 2.131890 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.36\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 800: 2.144835 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.59\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 900: 2.137388 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 1000: 2.148496 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.23\n",
      "================================================================================\n",
      "goee visiur lated reey epwn as that amel ch pwn and the latiius pasicar sreat the\n",
      "nfir  morsufn duch altist in ventisiste lientury and eebile sogy fore to spes mec\n",
      "qhcone four a faogs of a stare les butich hemcjl cobm orpy hotroad sim fut he roy\n",
      "ykty pest five mal one ninferes hemorer major the do deantvt goo en mo cunds seve\n",
      "smad cornun whet mospot a caniesa poloj nineme usiden nences war use yemtgs onei \n",
      "================================================================================\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 1100: 2.108819 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 1200: 2.095640 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.47\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1300: 2.089526 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 1400: 2.097608 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1500: 2.093540 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1600: 2.075209 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1700: 2.063696 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 1800: 2.050447 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 1900: 2.052698 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 2000: 2.038863 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "================================================================================\n",
      "yd the wolone colecterts me savossirband exeforet omessimprituld the exeur sondep\n",
      "ms two and asladips instichith its three comprollefmauntuza degyperiannot conenti\n",
      "vt the that ded femmating galoweanefrounationdes is atf the fouraas on unifent is\n",
      "pq eade meralord funder expanitern questure two zero exoruo exas form three diffu\n",
      "haves deven rrurts chard stersonson bakeny vaste to to cheon stry in the seven le\n",
      "================================================================================\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 2100: 2.049582 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 2200: 2.072649 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2300: 2.080368 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 2400: 2.059407 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2500: 2.062141 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 2600: 2.047112 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2700: 2.058558 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 2800: 2.059600 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 2900: 2.049886 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.80\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3000: 2.044856 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "================================================================================\n",
      "dp neys reakre ans to ter coment ne natentizenssica destor relogersee eget done r\n",
      " a sexegitneare on also ancelsirn putivents upawthere jectreing spess puqern rion\n",
      "ij jecone sme meterse demberland attle usky perfht teracrio eight word two four l\n",
      "mskyzs brive of tego betwearces mys with the dedip fork stertateric agaimme ired \n",
      "ing even the conr is two comestance thirs dyvctate sicly bormh to fris in ofsestr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 3100: 2.025409 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 3200: 2.018774 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 3300: 2.020231 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3400: 2.023078 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3500: 2.050093 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 3600: 2.032342 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 3700: 2.030119 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 3800: 2.034730 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3900: 2.040581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 4000: 2.034556 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "================================================================================\n",
      "ics the was monanhis the bent in for ozing andia gganes yay the one nam stme mani\n",
      "furnel pertion fourmy stass ton their sequot five mes proteastento letor ends nin\n",
      "equillauto is nons infor up hannet mary vated lone gate slan andarsonisomerat cul\n",
      "szset in and nifeet efht usx audt one zero and las of latent the aftestatel s to \n",
      "ive of the nerate bickpolide of the as and pojeloctjrge doukres one zero molets a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 4100: 2.016805 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 4200: 2.001875 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 4300: 2.013232 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 4400: 1.993913 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.12\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 4500: 2.034747 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4600: 2.027801 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 4700: 2.013670 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 4800: 2.018591 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 4900: 2.019964 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5000: 2.015537 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "================================================================================\n",
      "zlacurts laeversed abough was tracharaehide g dies bagorpoparch poema from six th\n",
      "muss six stabill and lare thi arj recan of and more inholowns four pat its in sou\n",
      "uw at eight of seal mor one sessuald the use alting bile  are otherro the curish \n",
      "om crampdcactrotles braman coverum races detry appoly alable coubsevence im for b\n",
      "cnog mollolow mlipans in harake cory itted for to ital are farger arger of the ei\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5100: 1.985763 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5200: 1.989261 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5300: 1.983999 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5400: 1.988973 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 5500: 1.997179 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.13\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 5600: 1.968937 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 5700: 1.968937 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5800: 1.986864 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 5900: 1.978688 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 6000: 1.969024 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.36\n",
      "================================================================================\n",
      "lqtfered one that biyaeleatos art has hano to aguz a nosmay asy hese arone zero c\n",
      "jzi quired beeman ming and ber in cluse bety the one agrime wor reactaran inced b\n",
      "ub loped in two six nine nine nine nine pointed have istancurne threa dograd ucke\n",
      "deegosaxt of suft zero inter beiro sent to preves concing in d one thing eoit wea\n",
      "wuothative main ost stenql layiirfb of publy one whic two in eled shol and stally\n",
      "================================================================================\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 6100: 1.964403 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 6200: 1.973884 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 6300: 1.974573 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 6400: 1.967169 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 6500: 1.947442 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6600: 1.999198 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6700: 1.973028 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 6800: 1.978437 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 6900: 1.965476 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 7000: 1.985523 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.55\n",
      "================================================================================\n",
      "s a gesuan of cails hothers the molutizecmmence the searly shirded deral thomist \n",
      "mging the advolingrafk bolence wicondomialy seven a they ext unity as the gen a p\n",
      "q moth in alrarmolon have her of pijundut in vistate shol preducomerial hang ince\n",
      "xwkod a the d saaer five eighters histirring to zero systal withorks physignalibe\n",
      "adly ground of there progir sevelocict and cantretation moves ropipty in the goza\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            feed.append(random_distribution())\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: {\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n",
      "Unexpected character: |\n"
     ]
    }
   ],
   "source": [
    "#Adapted from Seq2Seq Tutorial\n",
    "num_nodes = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_data_reversed = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size])\n",
    "    )\n",
    "    train_data_reversed.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size])\n",
    "    )\n",
    "  go_batch = np.array([char2vector(GO) for _ in range(batch_size)], dtype=np.float32)\n",
    "  eos_batch = np.array([char2vector(EOS) for _ in range(batch_size)], dtype=np.float32)\n",
    "  encoder_inputs = train_data[:num_unrollings]\n",
    "  decoder_inputs = train_data_reversed[-num_unrollings:]\n",
    "  train_labels = decoder_inputs + [eos_batch]\n",
    "\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in encoder_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "  \n",
    "  for i in [go_batch] + decoder_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  with tf.control_dependencies([\n",
    "    saved_output.assign(output),\n",
    "    saved_state.assign(state)]\n",
    "  ):\n",
    "    logits = [\n",
    "      tf.nn.xw_plus_b(tf.concat(0, output), w, b)\n",
    "      for output in outputs\n",
    "    ]\n",
    "    train_labels = [\n",
    "        tf.argmax(train_labels_batch, 1)\n",
    "        for train_labels_batch in train_labels\n",
    "    ]\n",
    "    weights = [\n",
    "        np.ones(shape=(batch_size,), dtype=np.float32)\n",
    "        for _ in range(len(logits))\n",
    "    ]\n",
    "    loss = tf.nn.seq2seq.sequence_loss(logits, train_labels, weights, vocabulary_size)\n",
    "\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    8.0,\n",
    "    global_step,\n",
    "    1500,\n",
    "    0.05,\n",
    "    staircase=True\n",
    "  )\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step\n",
    "  )\n",
    "\n",
    "  train_prediction = tf.nn.softmax(tf.concat(0, logits))\n",
    "  \n",
    "  sample_input = [\n",
    "    tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    for _ in range(num_unrollings)\n",
    "  ]\n",
    "  sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_outputs = []\n",
    "  prob_outputs = []\n",
    "  reset_sample_state = tf.group(\n",
    "    sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "  )\n",
    "  for i in sample_input:\n",
    "    sample_output, sample_state = lstm_cell(i, sample_output, sample_state)\n",
    "  decoder_input = np.array([char2vector(GO)], dtype=np.float32)\n",
    "  for i in range(num_unrollings):\n",
    "    sample_output, sample_state = lstm_cell(decoder_input, sample_output, sample_state)\n",
    "    prob_output = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    prob_outputs.append(prob_output)\n",
    "    output_char = tf.nn.softmax(prob_output)\n",
    "    decoder_input = output_char\n",
    "    sample_outputs.append(output_char)\n",
    "  sample_probabilities = tf.concat(0, prob_outputs)\n",
    "  sample_prediction_reversed = tf.concat(0, sample_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-498b3d950b2d>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.298398 learning rate: 8.000000\n",
      "Minibatch perplexity: 96258472350832106757233254088450905507366961152.00\n",
      "================================================================================\n",
      "wo\n",
      "or\n",
      "rk\n",
      "ki\n",
      "in\n",
      "================================================================================\n",
      "Average loss at step 100: 2.755019 learning rate: 8.000000\n",
      "Minibatch perplexity: 13371132332115259778411306329119359431261863509667386228736.00\n",
      "Average loss at step 200: 2.572642 learning rate: 8.000000\n",
      "Minibatch perplexity: 17101670299755865246798671546724116105457600452365352106459136.00\n",
      "Average loss at step 300: 2.466324 learning rate: 8.000000\n",
      "Minibatch perplexity: 4254254720010510104812856484244906539635485552170092748120392728576.00\n",
      "Average loss at step 400: 2.445473 learning rate: 8.000000\n",
      "Minibatch perplexity: 324814899796686735076343798068336767615144764902758050913690058752.00\n",
      "Average loss at step 500: 2.398246 learning rate: 8.000000\n",
      "Minibatch perplexity: 193824621474762139195256511872874543911000966129730728913409146880.00\n",
      "Average loss at step 600: 2.354207 learning rate: 8.000000\n",
      "Minibatch perplexity: 184495884819281651071811949757073507946747360842670385720390845161396942929920.00\n",
      "Average loss at step 700: 2.273792 learning rate: 8.000000\n",
      "Minibatch perplexity: 16773111192121349130210308829043543987324944307026618997202007606700277760.00\n",
      "Average loss at step 800: 2.217484 learning rate: 8.000000\n",
      "Minibatch perplexity: 5381111529971107721548756515311061293022371501645911898000929758059167744.00\n",
      "Average loss at step 900: 2.152443 learning rate: 8.000000\n",
      "Minibatch perplexity: 27378226489571295400572698720558123458617755175712002192823543106627436544.00\n",
      "Average loss at step 1000: 2.103160 learning rate: 8.000000\n",
      "Minibatch perplexity: 65580523208821422048979485758715426137039971684984442211726534834651136.00\n",
      "================================================================================\n",
      "ng\n",
      "g \n",
      " c\n",
      "cl\n",
      "la\n",
      "================================================================================\n",
      "Average loss at step 1100: 2.061776 learning rate: 8.000000\n",
      "Minibatch perplexity: 188963359989958220351105655434838878398191832031233015860161653034188800.00\n",
      "Average loss at step 1200: 2.042591 learning rate: 8.000000\n",
      "Minibatch perplexity: 3306933301241230314845259778226429887252733179169837875004663683979623268352.00\n",
      "Average loss at step 1300: 2.008283 learning rate: 8.000000\n",
      "Minibatch perplexity: 152699621879926551067997157070126839131463753775796094117255861213344290570240.00\n",
      "Average loss at step 1400: 2.017862 learning rate: 8.000000\n",
      "Minibatch perplexity: 18046095005539273140956157461552463685237691286100445554489541969415102628102144.00\n",
      "Average loss at step 1500: 2.021562 learning rate: 0.400000\n",
      "Minibatch perplexity: 135871647949774784958962534621052420060517970070635793800683310272005620028141469696.00\n",
      "Average loss at step 1600: 1.929047 learning rate: 0.400000\n",
      "Minibatch perplexity: 778405970023401840778715297711075213770647032991830297957770044481371050216296515224842768848037779341312.00\n",
      "Average loss at step 1700: 1.911567 learning rate: 0.400000\n",
      "Minibatch perplexity: 9747511428290359768327097083963884780705447764951095516643405154573561506048179923395790591892965726747623424.00\n",
      "Average loss at step 1800: 1.907123 learning rate: 0.400000\n",
      "Minibatch perplexity: 3462347167609972580036997494452444054391726800710572504871573775773848906307316543651831005338137330007538059444224.00\n",
      "Average loss at step 1900: 1.894650 learning rate: 0.400000\n",
      "Minibatch perplexity: 41969238070866290133848232554525250774386244437062759678354710597433462687979564342731302139269757598916036337336320.00\n",
      "Average loss at step 2000: 1.886038 learning rate: 0.400000\n",
      "Minibatch perplexity: 1215836992919058110048880778979676503915005643924779743669482535705900220687334621925711171223822079942603110904620413616128.00\n",
      "================================================================================\n",
      "as\n",
      "ss\n",
      "s \n",
      " r\n",
      "ra\n",
      "================================================================================\n",
      "Average loss at step 2100: 1.895062 learning rate: 0.400000\n",
      "Minibatch perplexity: 18002710578895575088200035265542575524433202454715904863437181710960165028618091603238856871086241885602402459203928064.00\n",
      "Average loss at step 2200: 1.890005 learning rate: 0.400000\n",
      "Minibatch perplexity: 4717374082376722229757714358895530800503423800778848215839734807438835163037925304499345633034177917836456605680953328664576.00\n",
      "Average loss at step 2300: 1.890490 learning rate: 0.400000\n",
      "Minibatch perplexity: 1459924709246350299679004579301254681501023275436021123016012896502542034554489631953969085030541185478544530557982200318394368.00\n",
      "Average loss at step 2400: 1.895583 learning rate: 0.400000\n",
      "Minibatch perplexity: 35811542973363351004192952433672304757265782101045320765534579612303661650345911538960331937435621761809771157802111986791612416.00\n",
      "Average loss at step 2500: 1.875757 learning rate: 0.400000\n",
      "Minibatch perplexity: 241609367186843731687336608077909298644584866706577862746310536066677121794977324748135282268947853773406497109166794064544137216.00\n",
      "Average loss at step 2600: 1.876273 learning rate: 0.400000\n",
      "Minibatch perplexity: 225595819439431691545233113544774165603281787888207875953625075155672396914559637251760465688389220557323988861940448333907623936.00\n",
      "Average loss at step 2700: 1.888314 learning rate: 0.400000\n",
      "Minibatch perplexity: 990217177331782853118089682610183274334293921788749580754868065137184063718475383877562561479447297137739381046136947477896495104.00\n",
      "Average loss at step 2800: 1.878036 learning rate: 0.400000\n",
      "Minibatch perplexity: 7621963847821965944766566748253694304171249880647550589581513729799235590269353162341279097671438433983468543533575722023350601464676352.00\n",
      "Average loss at step 2900: 1.877857 learning rate: 0.400000\n",
      "Minibatch perplexity: 160357612576936980508120640127263891704768058933679579740932513334817589906375742849095976266938905036878727457673444755578552320000.00\n",
      "Average loss at step 3000: 1.872063 learning rate: 0.020000\n",
      "Minibatch perplexity: 368401412800957878924127747200119857693600115361828952085809276832291695186293017368978181571311210600058046040623324263263636872495104.00\n",
      "================================================================================\n",
      "ad\n",
      "di\n",
      "ic\n",
      "ca\n",
      "al\n",
      "================================================================================\n",
      "Average loss at step 3100: 1.874279 learning rate: 0.020000\n",
      "Minibatch perplexity: 65184134747860229744519777187100345946463201529386104875945267005988766138652176660682410660892044077338161343560772420632093621485568.00\n",
      "Average loss at step 3200: 1.881935 learning rate: 0.020000\n",
      "Minibatch perplexity: 1067233465797444665974428057535914635330687426954597630753310161170748045009767354272977032448839081835949341158662813834684837920768.00\n",
      "Average loss at step 3300: 1.872139 learning rate: 0.020000\n",
      "Minibatch perplexity: 19283712295480216237089142967762137359416354856141438916305431981032635908632601171007270001487445309169654621981308244700953691237788841672704.00\n",
      "Average loss at step 3400: 1.860027 learning rate: 0.020000\n",
      "Minibatch perplexity: 52390944221671161942328134114568651366935192495120803716547740748718917634819835404352163365274446448843389199126315797442282985095168.00\n",
      "Average loss at step 3500: 1.905630 learning rate: 0.020000\n",
      "Minibatch perplexity: 627440104644212736825647577749781464809385302736732599899725347926306456576598573625989788792952300957847826238492082634945496061313024.00\n",
      "Average loss at step 3600: 1.874490 learning rate: 0.020000\n",
      "Minibatch perplexity: 26347379293225632874002861126841783322925178773425619309717101368216259282247463331198346468698968357637088662655986658230017023103731761152.00\n",
      "Average loss at step 3700: 1.890957 learning rate: 0.020000\n",
      "Minibatch perplexity: 15574916491272406455456555463755580625124656498097881578298800221900765321300118058642427816073828145150384647107910190533912674959360.00\n",
      "Average loss at step 3800: 1.871773 learning rate: 0.020000\n",
      "Minibatch perplexity: 10965862315371131997152701367521465904357121959950530628746964159000688386552534723573421217620823141419974171570755255207410532352.00\n",
      "Average loss at step 3900: 1.889218 learning rate: 0.020000\n",
      "Minibatch perplexity: 29149639534441662653675577679715501305880765797318535010344768861242577925540538385195889009362972449459300800129310640444958676674811002880.00\n",
      "Average loss at step 4000: 1.889264 learning rate: 0.020000\n",
      "Minibatch perplexity: 446603490009116551151005819811711666826385120604263920922140242488575663053305122671495811436000897034332906511653625465626167876784125640704.00\n",
      "================================================================================\n",
      "ls\n",
      "s \n",
      " i\n",
      "in\n",
      "nc\n",
      "================================================================================\n",
      "Average loss at step 4100: 1.855184 learning rate: 0.020000\n",
      "Minibatch perplexity: 13182898063873853097145494636234059612004172856978436640351278154279924612344508417964236214561912136266193450150423460635309303136256.00\n",
      "Average loss at step 4200: 1.859817 learning rate: 0.020000\n",
      "Minibatch perplexity: 277163106926979755279192170565536001478952674252858903590217717890249420111035615840219956081267062990964931764230678340910501903768485888.00\n",
      "Average loss at step 4300: 1.876704 learning rate: 0.020000\n",
      "Minibatch perplexity: 172739812041095317225399152746640158477806090401692790723513762535012762135791829460441907811141244447389748722834529515698325344342769664.00\n",
      "Average loss at step 4400: 1.900098 learning rate: 0.020000\n",
      "Minibatch perplexity: 122975025857877541504060035224573469149261207997238818549616303587637872901787011145667912578097026077184994928776625124219753070592.00\n",
      "Average loss at step 4500: 1.876932 learning rate: 0.001000\n",
      "Minibatch perplexity: 155833547945756672416479196660699173168720026484585604429227678844794904233628536205630798174992669352914103276935076506937816503537369088.00\n",
      "Average loss at step 4600: 1.878969 learning rate: 0.001000\n",
      "Minibatch perplexity: 75326784651172842828678101584187719798646667514883449949084520723197550452605939962188522169558052343950896557311345466820060512256.00\n",
      "Average loss at step 4700: 1.894226 learning rate: 0.001000\n",
      "Minibatch perplexity: 1565998918903337343218476364682192366698412098958531525069523318878172308831729867692370190753395442950067857619105021060618533286648479744.00\n",
      "Average loss at step 4800: 1.894301 learning rate: 0.001000\n",
      "Minibatch perplexity: 110463328518678650299114667423129526447821532992508104045896462509327279432647312907854265446871170606925410420047647675207904997146624.00\n",
      "Average loss at step 4900: 1.858111 learning rate: 0.001000\n",
      "Minibatch perplexity: 1180679096984584190098970850563100445275538565618195564329002582035893270766998580569894755107925024709016035331718648054330584727552.00\n",
      "Average loss at step 5000: 1.865792 learning rate: 0.001000\n",
      "Minibatch perplexity: 3477092241803728985386719155974892417597809181474006195648439886246480210048479972523496732004117208211846800713897466857073300996096.00\n",
      "================================================================================\n",
      "cl\n",
      "lu\n",
      "ud\n",
      "di\n",
      "in\n",
      "================================================================================\n",
      "Average loss at step 5100: 1.889442 learning rate: 0.001000\n",
      "Minibatch perplexity: 6389465931307716269640767745365177159613709823063688858312774268078664165746389234390290248400102346686647124984377358148573742497792.00\n",
      "Average loss at step 5200: 1.880150 learning rate: 0.001000\n",
      "Minibatch perplexity: 8102782102239855985999147084405589519419679724357675405764950465008621677687802953918143778443133596463989583424897536890276867571185614848.00\n",
      "Average loss at step 5300: 1.874764 learning rate: 0.001000\n",
      "Minibatch perplexity: 11202805342386925511689050673414962169887417383437680782870480415498413614261405139955615087962608087318714370757570382268314811772224667648.00\n",
      "Average loss at step 5400: 1.878226 learning rate: 0.001000\n",
      "Minibatch perplexity: 32869922776059316525282414400732235901178157146013388142582802176511714479484917211164764550167414740408056490392532998922640180135120076800.00\n",
      "Average loss at step 5500: 1.867033 learning rate: 0.001000\n",
      "Minibatch perplexity: 25301169363885766966869742215930989367718175523974168015314373306216058288255446281379675352870917517606105315899131337259525074598466945024.00\n",
      "Average loss at step 5600: 1.896068 learning rate: 0.001000\n",
      "Minibatch perplexity: 3806454230361444375909661614448695150153783343022401758899110366316210469160535767978572138243761346814904944819807269705030084185808699392.00\n",
      "Average loss at step 5700: 1.866956 learning rate: 0.001000\n",
      "Minibatch perplexity: 2407080709014826696166552707227905174408423631968366171729281662282900284860792218708339432411937020081161755122027678358730470913726218240.00\n",
      "Average loss at step 5800: 1.883763 learning rate: 0.001000\n",
      "Minibatch perplexity: 525132230610875429329833467391947545349850083176198948942536846227989953994306530261342588580501689635616041103226851611225897304064.00\n",
      "Average loss at step 5900: 1.901739 learning rate: 0.001000\n",
      "Minibatch perplexity: 523936077868053592898673680620595274914473836824499850513577575416697594445389315368288351769762772073738478418144750285332328933752832.00\n",
      "Average loss at step 6000: 1.880910 learning rate: 0.000050\n",
      "Minibatch perplexity: 942106424661354060658158123921745546283025026632491497119475354873306094084625898584304655445597795753995841728770640929611574157508608.00\n",
      "================================================================================\n",
      "ng\n",
      "g \n",
      " t\n",
      "th\n",
      "he\n",
      "================================================================================\n",
      "Average loss at step 6100: 1.871182 learning rate: 0.000050\n",
      "Minibatch perplexity: 1042738958293936425762917703028053374465754005911524049633748945126957249846361596920791148132996987830676766385084549691300904960.00\n",
      "Average loss at step 6200: 1.885669 learning rate: 0.000050\n",
      "Minibatch perplexity: 48192731123411004684518074525649185382568709782509157831615799833509100998957584886715492314113605867164527151283437711779024146693881856.00\n",
      "Average loss at step 6300: 1.885755 learning rate: 0.000050\n",
      "Minibatch perplexity: 78433242743519206176175715007046192297391219051091694608836322023844786288979606722883931228501177920337012664196802140428696074682630144.00\n",
      "Average loss at step 6400: 1.900586 learning rate: 0.000050\n",
      "Minibatch perplexity: 6074044374564109241378521253494277223340041244602217348406074175551314658651158059746711520440216930574329445915583997326038480530555535360.00\n",
      "Average loss at step 6500: 1.886200 learning rate: 0.000050\n",
      "Minibatch perplexity: 4894815855771544214886722899438440352535078622334275400270126749472852627037731967756456089030387671096545772868060769914860410503168.00\n",
      "Average loss at step 6600: 1.901468 learning rate: 0.000050\n",
      "Minibatch perplexity: 4465277724329047528843243623207300688094156179416780410723236223492914075279963493738681365727196448666349147011367435572971469445005312.00\n",
      "Average loss at step 6700: 1.877186 learning rate: 0.000050\n",
      "Minibatch perplexity: 72002216232786052300294035518581064541587212747617158824321605891241706118817967048223205174301288730124173392535824222573280798709253144576.00\n",
      "Average loss at step 6800: 1.879188 learning rate: 0.000050\n",
      "Minibatch perplexity: 508134277340668681856206838920203020261056013551290928017134828659080170298473781737436461557554625700725066390758210117625999494479872.00\n",
      "Average loss at step 6900: 1.892759 learning rate: 0.000050\n",
      "Minibatch perplexity: 10941899323386490695779676128624734819662818388363339615477750437461567670400670462323259757370679817252854581961960965383179431890124800.00\n",
      "Average loss at step 7000: 1.881801 learning rate: 0.000050\n",
      "Minibatch perplexity: 122992133731688311074079838857555229940027156721573454219652011161376854176918481388240992535254654645317532807298746228605343034769408.00\n",
      "================================================================================\n",
      "e \n",
      " d\n",
      "di\n",
      "ig\n",
      "gg\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    batches_reversed = train_reversed_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      feed_dict[train_data_reversed[i]] = batches_reversed[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate],\n",
    "      feed_dict=feed_dict\n",
    "    )\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if 0 < step:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr)\n",
    "      )\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches_reversed)[1:] + [eos_batch])\n",
    "      print(\n",
    "        'Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels)))\n",
    "      )\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = valid_batches.next()[:num_unrollings]\n",
    "          sentence = ''.join([characters(feed_i)[0] for feed_i in feed])\n",
    "          reset_sample_state.run()\n",
    "          print(sentence)\n",
    "        print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
